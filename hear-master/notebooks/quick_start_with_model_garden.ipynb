{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "u_OIwDav0A4W"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_oWMJLg0fLk"
      },
      "source": [
        "# Quick start with Model Garden - HeAR\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fhear%2Fmaster%2Fnotebooks%2Fquick_start_with_model_garden.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/hear/blob/master/notebooks/quick_start_with_model_garden.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsEU-DK7DJcv"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to use HeAR in Vertex AI to generate embeddings from audio clips of health-related sounds using two methods for getting predictions:\n",
        "\n",
        "* **Online predictions** are synchronous requests that are made to the endpoint deployed from Model Garden and are served with low latency. Online predictions are useful if the embeddings are being used in production. The cost for online prediction is based on the time a virtual machine spends waiting in an active state (an endpoint with a deployed model) to handle prediction requests.\n",
        "\n",
        "* **Batch predictions** are asynchronous requests that are run on a set number of audio clips specified in a single job. They are made directly to an uploaded model and do not use an endpoint deployed from Model Garden. Batch predictions are useful if you want to generate embeddings for a large number of clips for use in training and don't require low latency. The cost for batch prediction is based on the time a virtual machine spends running your prediction job.\n",
        "\n",
        "Vertex AI makes it easy to serve your model and make it accessible to the world. Learn more about [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform).\n",
        "\n",
        "### Objectives\n",
        "\n",
        "- Deploy HeAR to a Vertex AI Endpoint and get online predictions.\n",
        "- Upload HeAR to Vertex AI Model Registry and get batch predictions.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_iHV1RDA3C"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-nRhKI2hEpzn"
      },
      "outputs": [],
      "source": [
        "# @title Import packages and define common functions\n",
        "\n",
        "import base64\n",
        "import datetime\n",
        "import importlib\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "import numpy as np\n",
        "from google.cloud import aiplatform, storage\n",
        "from IPython.display import Audio, display\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "\n",
        "if not os.path.isdir(\"vertex-ai-samples\"):\n",
        "    ! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "SAMPLE_RATE = 16000  # Samples per second (Hz)\n",
        "CLIP_DURATION = 2  # Duration of the audio clip in seconds\n",
        "CLIP_LENGTH = SAMPLE_RATE * CLIP_DURATION  # Total number of samples\n",
        "\n",
        "\n",
        "def resample_audio_and_convert_to_mono(\n",
        "    audio_array: np.ndarray,\n",
        "    sampling_rate: float,\n",
        "    new_sampling_rate: float = SAMPLE_RATE,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Resamples an audio array and converts it to mono if it has multiple channels.\n",
        "\n",
        "    Args:\n",
        "      audio_array: A numpy array representing the audio data.\n",
        "      sampling_rate: The original sampling rate of the audio.\n",
        "      new_sampling_rate: Target sampling rate.\n",
        "\n",
        "    Returns:\n",
        "      resampled_audio_mono: A numpy array representing the resampled mono audio.\n",
        "    \"\"\"\n",
        "    # Convert to mono if it's multi-channel\n",
        "    if audio_array.ndim > 1:\n",
        "        audio_mono = np.mean(audio_array, axis=1)\n",
        "    else:\n",
        "        audio_mono = audio_array\n",
        "\n",
        "    # Resample\n",
        "    original_sample_count = audio_mono.shape[0]\n",
        "    new_sample_count = int(\n",
        "        round(original_sample_count * (new_sampling_rate / sampling_rate))\n",
        "    )\n",
        "    resampled_audio_mono = signal.resample(audio_mono, new_sample_count)\n",
        "\n",
        "    return resampled_audio_mono"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2_aYhzoEDMCf"
      },
      "outputs": [],
      "source": [
        "# @title Set up Google Cloud environment\n",
        "\n",
        "# @markdown #### Prerequisites\n",
        "\n",
        "# @markdown Make sure that:\n",
        "\n",
        "# @markdown 1. [Billing is enabled](https://cloud.google.com/billing/docs/how-to/modify-project) for your project.\n",
        "\n",
        "# @markdown 2. Either the Compute Engine API is enabled or you have the [Service Usage Admin](https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin) (`roles/serviceusage.serviceUsageAdmin`) role to enable the API.\n",
        "\n",
        "# @markdown 3. You have the [Storage Admin](https://cloud.google.com/iam/docs/understanding-roles#storage.admin) (`roles/storage.admin`) role to create and use Cloud Storage buckets.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown This section:\n",
        "\n",
        "# @markdown 1. Sets the default Google Cloud project and region, enables the Compute Engine API (if not already enabled), and initializes the Vertex AI API.\n",
        "\n",
        "# @markdown 2. Sets up a Cloud Storage bucket for storing prediction artifacts.\n",
        "# @markdown - A new bucket will automatically be created for you.\n",
        "# @markdown - [Optional] To use an existing bucket, specify the `gs://` bucket URI. The specified Cloud Storage bucket should be located in the same region as where the notebook was launched. Note that a multi-region bucket (e.g. \"us\") is not considered a match for a single region (e.g. \"us-central1\") covered by the multi-region range.\n",
        "\n",
        "# Get the default project ID.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Compute Engine API, if not already.\n",
        "print(\"Enabling Compute Engine API.\")\n",
        "! gcloud services enable compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "BUCKET_URI = \"\"  # @param {type:\"string\", placeholder:\"[Optional] Cloud Storage bucket URI\"}\n",
        "\n",
        "# Cloud Storage bucket for storing prediction artifacts.\n",
        "# A unique bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value of BUCKET_URI above.\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\":\n",
        "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gcloud storage buckets create --location {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    shell_output = ! gcloud storage buckets describe {BUCKET_NAME} | grep \"location:\" | sed \"s/location://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            f\"Bucket region {bucket_region} is different from notebook region {REGION}\"\n",
        "        )\n",
        "print(f\"Using this Cloud Storage Bucket: {BUCKET_URI}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CQxur5PzVop-"
      },
      "outputs": [],
      "source": [
        "# @title Prepare sample cough audio\n",
        "\n",
        "# @markdown The expected prediction input for HeAR is 2 second clips of audio sampled at 16kHz. This section prepares a clip using a sample cough audio file from Wikimedia Commons and stores it in the following formats to be used for prediction in the next sections:\n",
        "\n",
        "# @markdown 1. WAV audio file (`sample_cough.wav` in the Cloud Storage bucket `BUCKET_URI`)\n",
        "\n",
        "# @markdown 2. WAV bytes (in memory)\n",
        "\n",
        "# @markdown 3. Audio array (in memory)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "# Attribution: Ashe Kirk, CC0, via Wikimedia Commons\n",
        "audio_file_url = (\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/b/be/Woman_coughing_three_times.wav\"\n",
        ")\n",
        "\n",
        "! wget -nc {audio_file_url}\n",
        "filename = os.path.basename(audio_file_url)\n",
        "with open(filename, \"rb\") as f:\n",
        "    original_sampling_rate, audio_array = wavfile.read(f)\n",
        "audio_array = resample_audio_and_convert_to_mono(audio_array, original_sampling_rate)\n",
        "# This index corresponds to the start of a health acoustic event\n",
        "# (e.g. cough in this case) and was determined by hand. In practice,\n",
        "# you would need a detector.\n",
        "start = 0\n",
        "clip = audio_array[start : start + CLIP_LENGTH]\n",
        "# Pad clip with zeros if less than the required CLIP_LENGTH\n",
        "if len(clip) < CLIP_LENGTH:\n",
        "    clip = np.pad(clip, (0, CLIP_LENGTH - len(clip)))\n",
        "display(Audio(clip, rate=SAMPLE_RATE))\n",
        "\n",
        "# Save the audio data to a BytesIO object (in-memory file)\n",
        "bytes_io = io.BytesIO()\n",
        "wavfile.write(bytes_io, SAMPLE_RATE, clip)\n",
        "\n",
        "# Sample cough audio prepared as a 2 second clip sampled at 16kHz and stored in\n",
        "# the formats below to be used for prediction in the next sections\n",
        "SAMPLE_AUDIO_ARRAY = clip.tolist()\n",
        "SAMPLE_WAV_BYTES = bytes_io.getvalue()\n",
        "SAMPLE_GCS_URI = f\"{BUCKET_URI}/sample_cough.wav\"\n",
        "\n",
        "# Upload the audio file to Cloud Storage\n",
        "storage_client = storage.Client()\n",
        "blob = storage.blob.Blob.from_string(SAMPLE_GCS_URI, client=storage_client)\n",
        "blob.upload_from_string(SAMPLE_WAV_BYTES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpGxMQxndGcG"
      },
      "source": [
        "## Get online predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "55wdOLrEK8kg"
      },
      "outputs": [],
      "source": [
        "# @title Import deployed model\n",
        "\n",
        "# @markdown To get [online predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions), you will need a HeAR [Vertex AI Endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment) that has been deployed from Model Garden. If you have not already done so, go to the [HeAR model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/hear) in Model Garden and click \"Deploy\" to deploy the model.\n",
        "\n",
        "# @markdown This section gets the Vertex AI Endpoint resource that you deployed from Model Garden to use for online predictions.\n",
        "\n",
        "# @markdown Fill in the endpoint ID and region below. You can find your deployed endpoint on the [Vertex AI online prediction page](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "\n",
        "ENDPOINT_ID = \"\"  # @param {type: \"string\", placeholder:\"e.g. 123456789\"}\n",
        "ENDPOINT_REGION = \"\"  # @param {type: \"string\", placeholder:\"e.g. us-central1\"}\n",
        "\n",
        "endpoints[\"endpoint\"] = aiplatform.Endpoint(\n",
        "    endpoint_name=ENDPOINT_ID,\n",
        "    project=PROJECT_ID,\n",
        "    location=ENDPOINT_REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5487N3VQxqi_"
      },
      "source": [
        "### Predict\n",
        "\n",
        "You can send [online prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions) requests to the endpoint with audio clips of health-related sounds (2 seconds sampled at 16kHz) to generate embeddings.\n",
        "\n",
        "The following examples demonstrate using HeAR to generate embeddings from:\n",
        "\n",
        "* A WAV audio file stored in [Cloud Storage](https://cloud.google.com/storage/docs)\n",
        "* Base64-encoded WAV bytes\n",
        "* Array of 32000 floats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "trU5YDBEHwn-"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate an embedding from an audio file in Cloud Storage\n",
        "\n",
        "# @markdown This section shows an example of generating an embedding using a sample audio file stored in Cloud Storage.\n",
        "\n",
        "# @markdown The prediction request instance contains the following fields:\n",
        "# @markdown - `gcs_uri`: `gs://` URI specifying the location of a WAV audio file stored in Cloud Storage\n",
        "# @markdown - `bearer_token`: Bearer token used to access data in Cloud Storage (optional for public buckets)\n",
        "\n",
        "# @markdown You can specify `GCS_URI` below to use your own data.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "GCS_URI = \"\"  # @param {type:\"string\", placeholder:\"Cloud Storage file URI (leave blank to use sample data)\"}\n",
        "\n",
        "if not GCS_URI:\n",
        "    GCS_URI = SAMPLE_GCS_URI\n",
        "\n",
        "bearer_token = ! gcloud auth print-access-token\n",
        "bearer_token = bearer_token[0]\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"gcs_uri\": GCS_URI,\n",
        "        \"bearer_token\": bearer_token,\n",
        "    },\n",
        "]\n",
        "\n",
        "response = endpoints[\"endpoint\"].predict(instances=instances)\n",
        "predictions = response.predictions\n",
        "\n",
        "embedding_vector = np.array(predictions[0][\"embedding\"]).flatten()\n",
        "print(\"Size of embedding vector:\", len(embedding_vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Om6EWZ6Bxf0Q"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate an embedding from WAV bytes\n",
        "\n",
        "# @markdown This section shows an example of generating an embedding from sample WAV bytes.\n",
        "\n",
        "# @markdown The prediction request instance contains the following field:\n",
        "# @markdown - `input_bytes`: Base-64 encoded WAV bytes\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "instances = [{\"input_bytes\": base64.b64encode(SAMPLE_WAV_BYTES).decode(\"utf-8\")}]\n",
        "\n",
        "response = endpoints[\"endpoint\"].predict(instances=instances)\n",
        "predictions = response.predictions\n",
        "\n",
        "embedding_vector = np.array(predictions[0][\"embedding\"]).flatten()\n",
        "print(\"Size of embedding vector:\", len(embedding_vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yxSztq6VMbjX"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate an embedding from an audio array\n",
        "\n",
        "# @markdown This section shows an example of generating an embedding from a sample audio array.\n",
        "\n",
        "# @markdown The prediction request instance contains the following field:\n",
        "# @markdown - `input_array`: Array of 32000 floats\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "instances = [{\"input_array\": SAMPLE_AUDIO_ARRAY}]\n",
        "\n",
        "response = endpoints[\"endpoint\"].predict(instances=instances)\n",
        "predictions = response.predictions\n",
        "\n",
        "embedding_vector = np.array(predictions[0][\"embedding\"]).flatten()\n",
        "print(\"Size of embedding vector:\", len(embedding_vector))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GjsXPnATk2B"
      },
      "source": [
        "## Get batch predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9A6-24lJVM8Y"
      },
      "outputs": [],
      "source": [
        "# @title Upload model to Vertex AI Model Registry\n",
        "\n",
        "# @markdown To get [batch predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions), you must first upload the prebuilt HeAR model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction). Batch prediction requests are made directly to a model in Model Registry without deploying to an endpoint.\n",
        "\n",
        "MODEL_ID = \"hear\"\n",
        "MODEL_ARTIFACT_URI = \"gs://vertex-model-garden-restricted-us/hear\"\n",
        "\n",
        "# The pre-built serving docker image.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/health-ai-hear.cpu.1-0.ubuntu2004.py312.tf218:20250311-1800-rc0\"\n",
        "\n",
        "\n",
        "def upload_model(model_name: str, artifact_uri: str) -> aiplatform.Model:\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        artifact_uri=artifact_uri,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "models[\"model\"] = upload_model(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=MODEL_ID),\n",
        "    artifact_uri=MODEL_ARTIFACT_URI,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_T9UDGV5TbrR"
      },
      "outputs": [],
      "source": [
        "# @title Set the service account for batch prediction\n",
        "\n",
        "# @markdown This section gets the [Compute Engine default service account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) which will be used to run the batch prediction jobs.\n",
        "\n",
        "# @markdown Make sure that you have the [Service Account User](https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountUser) (`roles/iam.serviceAccountUser`) role on either the project or the Compute Engine default service account.\n",
        "\n",
        "# Service account used for running the prediction container.\n",
        "# Gets the Compute Engine default service account. If you prefer using your own\n",
        "# custom service account, change the value of SERVICE_ACCOUNT below.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this service account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ZyqSb0pBvP"
      },
      "source": [
        "### Predict\n",
        "\n",
        "You can send [batch prediction requests](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#request_a_batch_prediction) to the model using a [JSON Lines](https://jsonlines.org/) file to specify a list of input instances with audio clips of health-related sounds (2 seconds sampled at 16kHz) to generate embeddings. For more details on configuring batch prediction jobs, see how to [format your input data](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#input_data_requirements) and [choose compute settings](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#choose_machine_type_and_replica_count).\n",
        "\n",
        "The following examples demonstrate using HeAR to generate embeddings in batch from:\n",
        "\n",
        "* WAV audio files stored in [Cloud Storage](https://cloud.google.com/storage/docs)\n",
        "* Base64-encoded WAV bytes\n",
        "* Arrays of 32000 floats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-5BC3KlAJXtj"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate embeddings in batch from audio files in Cloud Storage\n",
        "\n",
        "# @markdown This section shows an example of generating embeddings in batch using sample audio files stored in Cloud Storage.\n",
        "\n",
        "# @markdown Each line in the input JSON Lines file is a prediction request instance that contains the following field:\n",
        "# @markdown - `gcs_uri`: `gs://` URI specifying the location of a WAV audio file stored in Cloud Storage\n",
        "\n",
        "# @markdown You can specify `GCS_URIS` below use your own data.\n",
        "\n",
        "# @markdown **Note:** The custom service account used to launch the batch prediction job must have permission to read the data from Cloud Storage.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "# Comma-separated list of Cloud Storage URIs\n",
        "GCS_URIS = \"\"  # @param {type:\"string\", placeholder:\"Comma-separated list of Cloud Storage file URIs (leave blank to use sample data)\"}\n",
        "\n",
        "if not GCS_URIS:\n",
        "    gcs_uris_list = [SAMPLE_GCS_URI, SAMPLE_GCS_URI]\n",
        "    # Grant the custom service account permission to read data from Cloud Storage BUCKET_URI\n",
        "    ! gcloud storage buckets add-iam-policy-binding {BUCKET_URI} \\\n",
        "        --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "        --role=roles/storage.objectViewer\n",
        "else:\n",
        "    gcs_uris_list = GCS_URIS.split(\",\")\n",
        "\n",
        "batch_predict_instances = [{\"gcs_uri\": uri} for uri in gcs_uris_list]\n",
        "\n",
        "# Write instances to JSON Lines file\n",
        "os.makedirs(\"batch_predict_input\", exist_ok=True)\n",
        "instances_filename = \"gcs_instances.jsonl\"\n",
        "with open(f\"batch_predict_input/{instances_filename}\", \"w\") as f:\n",
        "   for line in batch_predict_instances:\n",
        "       json_str = json.dumps(line)\n",
        "       f.write(json_str)\n",
        "       f.write(\"\\n\")\n",
        "\n",
        "# Copy the file to Cloud Storage\n",
        "batch_predict_prefix = f\"batch-predict-{MODEL_ID}\"\n",
        "! gcloud storage cp ./batch_predict_input/{instances_filename} {BUCKET_URI}/{batch_predict_prefix}/input/{instances_filename}\n",
        "\n",
        "batch_predict_job_name = common_util.get_job_name_with_datetime(prefix=f\"batch-predict-{MODEL_ID}\")\n",
        "\n",
        "gcs_batch_predict_job = models[\"model\"].batch_predict(\n",
        "    job_display_name=batch_predict_job_name,\n",
        "    gcs_source=os.path.join(BUCKET_URI, batch_predict_prefix, f\"input/{instances_filename}\"),\n",
        "    gcs_destination_prefix=os.path.join(BUCKET_URI, batch_predict_prefix, \"output\"),\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "gcs_batch_predict_job.wait()\n",
        "\n",
        "print(gcs_batch_predict_job.display_name)\n",
        "print(gcs_batch_predict_job.resource_name)\n",
        "print(gcs_batch_predict_job.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i2CQ0rfUqahC"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate embeddings in batch from WAV bytes\n",
        "\n",
        "# @markdown This section shows an example of generating embeddings in batch from sample WAV bytes.\n",
        "\n",
        "# @markdown Each line in the input JSON Lines file is a prediction request instance that contains the following field:\n",
        "# @markdown - `input_bytes`: Base-64 encoded WAV bytes\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "batch_predict_instances = [\n",
        "    {\"input_bytes\": base64.b64encode(SAMPLE_WAV_BYTES).decode(\"utf-8\")},\n",
        "    {\"input_bytes\": base64.b64encode(SAMPLE_WAV_BYTES).decode(\"utf-8\")},\n",
        "]\n",
        "\n",
        "# Write instances to JSON Lines file\n",
        "os.makedirs(\"batch_predict_input\", exist_ok=True)\n",
        "instances_filename = \"bytes_instances.jsonl\"\n",
        "with open(f\"batch_predict_input/{instances_filename}\", \"w\") as f:\n",
        "    for line in batch_predict_instances:\n",
        "        json_str = json.dumps(line)\n",
        "        f.write(json_str)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Copy the file to Cloud Storage\n",
        "batch_predict_prefix = f\"batch-predict-{MODEL_ID}\"\n",
        "! gcloud storage cp ./batch_predict_input/{instances_filename} {BUCKET_URI}/{batch_predict_prefix}/input/{instances_filename}\n",
        "\n",
        "batch_predict_job_name = common_util.get_job_name_with_datetime(\n",
        "    prefix=f\"batch-predict-{MODEL_ID}\"\n",
        ")\n",
        "\n",
        "bytes_batch_predict_job = models[\"model\"].batch_predict(\n",
        "    job_display_name=batch_predict_job_name,\n",
        "    gcs_source=os.path.join(\n",
        "        BUCKET_URI, batch_predict_prefix, f\"input/{instances_filename}\"\n",
        "    ),\n",
        "    gcs_destination_prefix=os.path.join(BUCKET_URI, batch_predict_prefix, \"output\"),\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "bytes_batch_predict_job.wait()\n",
        "\n",
        "print(bytes_batch_predict_job.display_name)\n",
        "print(bytes_batch_predict_job.resource_name)\n",
        "print(bytes_batch_predict_job.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1am5O4Ev0Y7S"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate embeddings in batch from audio arrays\n",
        "\n",
        "# @markdown This section shows an example of generating embeddings in batch from sample audio arrays.\n",
        "\n",
        "# @markdown Each line in the input JSON Lines file is a prediction request instance that contains the following field:\n",
        "# @markdown - `input_array`: Array of 32000 floats\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "batch_predict_instances = [\n",
        "    {\"input_array\": SAMPLE_AUDIO_ARRAY},\n",
        "    {\"input_array\": SAMPLE_AUDIO_ARRAY},\n",
        "]\n",
        "\n",
        "# Write instances to JSON Lines file\n",
        "os.makedirs(\"batch_predict_input\", exist_ok=True)\n",
        "instances_filename = \"bytes_instances.jsonl\"\n",
        "with open(f\"batch_predict_input/{instances_filename}\", \"w\") as f:\n",
        "    for line in batch_predict_instances:\n",
        "        json_str = json.dumps(line)\n",
        "        f.write(json_str)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Copy the file to Cloud Storage\n",
        "batch_predict_prefix = f\"batch-predict-{MODEL_ID}\"\n",
        "! gcloud storage cp ./batch_predict_input/{instances_filename} {BUCKET_URI}/{batch_predict_prefix}/input/{instances_filename}\n",
        "\n",
        "batch_predict_job_name = common_util.get_job_name_with_datetime(\n",
        "    prefix=f\"batch-predict-{MODEL_ID}\"\n",
        ")\n",
        "\n",
        "array_batch_predict_job = models[\"model\"].batch_predict(\n",
        "    job_display_name=batch_predict_job_name,\n",
        "    gcs_source=os.path.join(\n",
        "        BUCKET_URI, batch_predict_prefix, f\"input/{instances_filename}\"\n",
        "    ),\n",
        "    gcs_destination_prefix=os.path.join(BUCKET_URI, batch_predict_prefix, \"output\"),\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "array_batch_predict_job.wait()\n",
        "\n",
        "print(array_batch_predict_job.display_name)\n",
        "print(array_batch_predict_job.resource_name)\n",
        "print(array_batch_predict_job.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NhAN2phRSlL0"
      },
      "outputs": [],
      "source": [
        "# @title #### Get prediction results\n",
        "\n",
        "# @markdown This section shows an example of [retrieving batch prediction results](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#retrieve_batch_prediction_results) from the JSON Lines file(s) in the output Cloud Storage location.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "\n",
        "def download_gcs_files_as_json(gcs_files_prefix):\n",
        "    \"\"\"Download specified files from Cloud Storage and convert content to JSON.\"\"\"\n",
        "    lines = []\n",
        "    client = storage.Client()\n",
        "    bucket = storage.bucket.Bucket.from_string(BUCKET_NAME, client)\n",
        "    blobs = bucket.list_blobs(prefix=gcs_files_prefix)\n",
        "    for blob in blobs:\n",
        "        with blob.open(\"r\") as f:\n",
        "            for line in f:\n",
        "                lines.append(json.loads(line))\n",
        "    return lines\n",
        "\n",
        "\n",
        "# Get results from the first batch prediction job (with Cloud Storage inputs)\n",
        "# You can replace this variable to get results from another batch prediction job\n",
        "batch_predict_job = gcs_batch_predict_job\n",
        "batch_predict_output_dir = batch_predict_job.output_info.gcs_output_directory\n",
        "batch_predict_output_files_prefix = os.path.join(\n",
        "    batch_predict_output_dir.replace(f\"{BUCKET_NAME}/\", \"\"), \"prediction.results\"\n",
        ")\n",
        "batch_predict_results = download_gcs_files_as_json(\n",
        "    gcs_files_prefix=batch_predict_output_files_prefix\n",
        ")\n",
        "\n",
        "# Display first two batch prediction results\n",
        "for i, line in enumerate(batch_predict_results[:2]):\n",
        "    embedding_vector = np.array(line[\"prediction\"][\"embedding\"]).flatten()\n",
        "    print(f\"Size of embedding vector {i}:\", len(embedding_vector))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFRiJ2o_LhoE"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/hear/blob/master/notebooks) to learn what else you can do with the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQMUmDYr6O_O"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iOSY_r6mHYui"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "quick_start_with_model_garden.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
