{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt64XBHqJiYo"
      },
      "source": [
        "~~~markdown\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# HeAR Event Detector Demo\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://colab.research.google.com/github/google-health/hear/blob/master/notebooks/hear_event_detector_demo.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"\u003e\u003cbr\u003e Run in Google Colab\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e  \n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/google-health/hear/blob/master/notebooks/hear_event_detector_demo.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://huggingface.co/google/hear\"\u003e\n",
        "      \u003cimg alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"\u003e\u003cbr\u003e View on HuggingFace\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n",
        "\n",
        "\n",
        "This Colab notebook demonstrates using the HeAR (Health Acoustic Representations) model along with the included Health Event Detectors directly from Hugging Face, to identify audio clips with relevent health sounds such as coughing, breathing or sneezing, then create and utilize embeddings from this subset of health-related audio clips.\n",
        "\n",
        "This notebook is similar to `train_data_efficient_classifier.ipynb` and also uses the small [Wikimedia Commons](https://commons.wikimedia.org/wiki/Commons:Welcome) dataset of relevant health sounds. In this example the audio files are reduced to a smaller subset of clips using the event detector to identify clips containing interesting health sounds to embed with HeAR.\n",
        "\n",
        "\n",
        "\n",
        "#### This notebook demonstrates:\n",
        "\n",
        "1.  Loading all supported Hugging Face Models (HeAR, Event Detector and Frontend).\n",
        "\n",
        "2.  Detecting 2-second clips within the [Wikimedia Commons](https://commons.wikimedia.org/wiki/Commons:Welcome) dataset with high probability of containing one or more of the supported event detection labels, then generating HeAR embedddings for these clips.\n",
        "\n",
        "3.  Finding the most similar audio files to a given query audio file based on the Cosine Similarity between the respective HeAR embeddings of the audio files.\n",
        "\n",
        "4. Optimizing the event detectors and frontend for low latency on-device usage using TFLite to support large scale feature event detection or feature generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uZFXCSuqr1V"
      },
      "source": [
        "# Authenticate with HuggingFace, skip if you have a HF_TOKEN secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-5Tj0uqS3dI"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub.utils import HfFolder\n",
        "\n",
        "if HfFolder.get_token() is None:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mubK5MmPQcUS"
      },
      "source": [
        "# Clone HuggingFace repository snapshot\n",
        "\n",
        "This will store the HeAR and event detector models in local cache so they can be loaded later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjfuNYeZQJdc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", tf.keras.__version__)\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "hugging_face_repo = \"google/hear\"\n",
        "local_snapshot_path = snapshot_download(repo_id=hugging_face_repo)\n",
        "print(f\"Saved {hugging_face_repo} to {local_snapshot_path}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO-Z5BOtj3D1"
      },
      "source": [
        "# Download Audio Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZnJwzzj3D1"
      },
      "source": [
        " Wiki Commons\n",
        "https://commons.wikimedia.org/wiki/Category:Coughing_audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ55XsJsj3D2"
      },
      "outputs": [],
      "source": [
        "# @title Download Public Domain Cough Examples to Notebook\n",
        "import os\n",
        "import subprocess\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# More examples: https://commons.wikimedia.org/wiki/Category:Coughing_audio\n",
        "wiki_cough_file_urls = [\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/c/cc/Man_coughing.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/6/6a/Cough_1.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/d/d9/Cough_2.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/b/be/Woman_coughing_three_times.wav',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/d/d0/Sneezing.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/e/ef/Laughter_and_clearing_voice.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/c/c6/Laughter.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/1/1c/Knocking_on_wood_or_door.ogg',\n",
        "]\n",
        "\n",
        "# Download the files.\n",
        "files_map = {}  # file name to file path map\n",
        "for url in wiki_cough_file_urls:\n",
        "  filename = os.path.basename(urlparse(url).path)\n",
        "  print(f'Downloading {filename}...')\n",
        "  res = subprocess.run(['wget', '-nv', '-O', filename, url], capture_output=True, text=True)\n",
        "  if res.returncode != 0:\n",
        "      print(f\"  Download failed. Return code: {res.returncode}\\nError: {res.stderr}\")\n",
        "  files_map[filename] = url\n",
        "print(f'\\nLocal Files:\\n{os.listdir():}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Eyu4VIsbUB"
      },
      "source": [
        "# Load Models and Run Inference\n",
        "\n",
        "### HeAR Model\n",
        "\n",
        "The HeAR model uses a powerful [ViT](https://huggingface.co/docs/transformers/en/model_doc/vit) backbone and generates rich 512 length embeddings from a 2 second single channel 16kHz audio clip. See the [HuggingFace Model Card](https://huggingface.co/google/hear) for more details.\n",
        "\n",
        "### Event Detector Models\n",
        "\n",
        "The event detector models use the efficient [MobileNet-V3](https://huggingface.co/docs/timm/en/models/mobilenet-v3) backbone paired with our custom TensorFlow spectrogram frontend.\n",
        "\n",
        "As with HeAR, these models expect a 2 second single channel 16kHz audio clip and output 8 detection probability scores for the following labels:\n",
        "\n",
        "```\n",
        "['Cough', 'Snore', 'Baby Cough', 'Breathe', 'Sneeze','Throat Clear', 'Laugh', 'Speech']\n",
        "```\n",
        "\n",
        "The event detector has two size variants which can be used interchangeably depending on the use-case.\n",
        "\n",
        "*  `event_detector_small/` is based on [MobileNetV3Small](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Small) with approximately **1M** parameters (3.60 MB)\n",
        "\n",
        "*  `event_detector_large/` is based on [MobileNetV3Large](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Large) with approximately **3M** parameters (11.46 MB)\n",
        "\n",
        "#### Spectrogram Frontend\n",
        "\n",
        "Our event detectors are fused with a custom, on-device optimized spectrogram frontend which efficiently converts 2 seconds of 16kHz audio into [PCEN](https://research.google/pubs/trainable-frontend-for-robust-and-far-field-keyword-spotting) scaled [Mel-spectrogram](https://huggingface.co/learn/audio-course/en/chapter1/audio_data#mel-spectrogram) features with 200 time steps and 48 Mel-frequency bins.\n",
        "\n",
        "*  `spectrogram_frontend/` is based on the PCEN implementation from [LEAF](https://research.google/blog/leaf-a-learnable-frontend-for-audio-classification/) and has only **5k** non-trainable parameters (18.56 KB) which are frozen and not configurable.\n",
        "\n",
        "We provide a standalone version of this frontend so that features can be pre-computed for new event detector training applications. See **Extract Batch Frontend Spectrogram Features** section for usage examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQVAxkBHZ_JO"
      },
      "outputs": [],
      "source": [
        "# @title Load HeAR, Event Detector and Frontend Models\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "\n",
        "# Constants for all included models, each input should be 32,000 samples.\n",
        "SAMPLE_RATE = 16000\n",
        "CLIP_DURATION = 2\n",
        "\n",
        "# Select event detector variant\n",
        "EVENT_DETECTOR = \"event_detector_small\" # @param [\"event_detector_large\", \"event_detector_small\"]\n",
        "# Included event detectors are trained to doutput detection probabilities in this order.\n",
        "LABEL_LIST =  ['Cough', 'Snore', 'Baby Cough', 'Breathe', 'Sneeze', 'Throat Clear', 'Laugh', 'Speech']\n",
        "\n",
        "# HeAR Embedding Model\n",
        "print(f\"\\nLoading HeAR model\")\n",
        "hear_model = from_pretrained_keras(local_snapshot_path)\n",
        "hear_infer = hear_model.signatures[\"serving_default\"]\n",
        "\n",
        "# Event detector models and frontend are nested in the \"event_detector/\" folder.\n",
        "# Detector Frontend Model for efficiently computing spectrogram feature\n",
        "frontend_path = os.path.join(\"event_detector/\", \"spectrogram_frontend\")\n",
        "print(f\"\\nLoading frontend model from: {frontend_path}\")\n",
        "frontend_model = from_pretrained_keras(\n",
        "    os.path.join(local_snapshot_path, frontend_path)\n",
        ")\n",
        "\n",
        "# Detector Model based on size variant selection, frontend included\n",
        "event_detector_path = os.path.join(\"event_detector/\", EVENT_DETECTOR)\n",
        "print(f\"\\nLoading detector model from: {event_detector_path}\")\n",
        "event_detector = from_pretrained_keras(\n",
        "    os.path.join(local_snapshot_path, event_detector_path)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33G4zKJHjGGc"
      },
      "outputs": [],
      "source": [
        "# @title Plot Helpers\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "\n",
        "# Suppress the specific warning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"soundfile\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"librosa\")\n",
        "\n",
        "def plot_waveform(sound, sr, title, figsize=(12, 4), color='blue', alpha=0.7):\n",
        "  \"\"\"Plots the waveform of the audio using librosa.display.\"\"\"\n",
        "  plt.figure(figsize=figsize)\n",
        "  librosa.display.waveshow(sound, sr=sr, color=color, alpha=alpha)\n",
        "  plt.title(f\"{title}\\nshape={sound.shape}, sr={sr}, dtype={sound.dtype}\")\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Amplitude\")\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_spectrogram(sound, sr, title, figsize=(12, 4), n_fft=2048, hop_length=256, n_mels=128, cmap='nipy_spectral'):\n",
        "  \"\"\"Plots the Mel spectrogram of the audio using librosa.\"\"\"\n",
        "  plt.figure(figsize=figsize)\n",
        "  mel_spectrogram = librosa.feature.melspectrogram(y=sound, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "  log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "  librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap=cmap)\n",
        "  plt.title(f\"{title} - Mel Spectrogram\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dJKyHXznPgg"
      },
      "outputs": [],
      "source": [
        "# @title Event Detector Plot Helpers\n",
        "\n",
        "def plot_frontend_feature(\n",
        "    frontend_feature: np.ndarray,\n",
        "    title: str,\n",
        "    figsize: tuple[int, int] = (12, 4),\n",
        "    cmap: str = 'nipy_spectral',\n",
        ") -\u003e None:\n",
        "  \"\"\"Plots the frontend spectrogram input feature.\n",
        "\n",
        "  Args:\n",
        "    frontend_feature: The event detector frontend feature as a 2D NumPy array\n",
        "      with shape (number of time steps, number of frequency bins). The default\n",
        "      shape for the included event detectors is (200, 48), which represents a\n",
        "      2-second audio clip with 48 frequency bins.\n",
        "    title: The title prefix of the plot.\n",
        "    figsize: Optional size of the figure.\n",
        "    cmap: Optional colormap to use.\n",
        "  \"\"\"\n",
        "  # Frontend features are typically rotated when fed into the model\n",
        "  # for spectrogram visualization it is more standard for x axis to be time\n",
        "  audio_spectrogram = np.rot90(frontend_feature)\n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.imshow(audio_spectrogram, aspect='auto', cmap=cmap)\n",
        "  plt.title(f\"{title} - Frontend PCEN Mel Spectrogram\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def plot_detection_scores(\n",
        "    scores_batch: np.ndarray,\n",
        "    label_list: list[str],\n",
        "    title: str,\n",
        "    figsize: tuple[int, int] = (12, 4),\n",
        "    cmap: str = 'nipy_spectral',\n",
        ") -\u003e None:\n",
        "  \"\"\"Plots per-label detection scores for batch sequentially with a consistent color scale.\n",
        "\n",
        "  Args:\n",
        "    scores_batch: The event detection scores as a 2D NumPy array with shape\n",
        "      (number of clips, number of labels). Where number of labels should match\n",
        "      the length of the label_list, which is 8 for the included event detectors.\n",
        "    label_list: A list of labels representing the event detector classes.\n",
        "    title: The title prefix of the plot.\n",
        "    figsize: Optional size of the figure.\n",
        "    cmap: Optional colormap to use.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=figsize)\n",
        "  scores_img = np.transpose(scores_batch)\n",
        "  # Explicitly set the color limits for imshow\n",
        "  im = plt.imshow(scores_img, aspect='auto', cmap=cmap, vmin=0, vmax=1)\n",
        "  # Set up the 'y' label axis\n",
        "  plt.yticks(\n",
        "      np.arange(len(label_list)), [l.replace(' ', '\\n') for l in label_list]\n",
        "  )\n",
        "  # Add horizontal grid lines between labels\n",
        "  for i in range(1, scores_img.shape[0]):\n",
        "    plt.axhline(y=i - 0.5, color='gray', linestyle='--')\n",
        "  plt.grid(axis='y', which='major', color='white', alpha=0)\n",
        "  # Setup the 'x' time axis\n",
        "  n_clips = scores_img.shape[1]\n",
        "  plt.xticks(np.arange(n_clips), [f'Clip {i+1}' for i in range(n_clips)])\n",
        "  plt.xlabel(\"Time Step\")\n",
        "  # Add vertical grid lines between time steps\n",
        "  for j in range(1, n_clips):\n",
        "    plt.axvline(x=j - 0.5, color='gray', linestyle='--')\n",
        "  plt.title(f\"{title} - Sound Event Detections\")\n",
        "  # Add colorbar with a consistent scale from 0 to 1\n",
        "  plt.colorbar(im, ticks=[0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYbz904QXio2"
      },
      "outputs": [],
      "source": [
        "# @title Load Audio and Generate HeAR Embeddings\n",
        "%%time\n",
        "\n",
        "# Audio display options\n",
        "SHOW_WAVEFORM = False\n",
        "SHOW_SPECTROGRAM = False\n",
        "SHOW_PLAYER = True\n",
        "SHOW_DETECTION_SCORES = True\n",
        "COLORMAP = \"Blues\"\n",
        "\n",
        "# Keep clips with high detection scores for these respiratory labels\n",
        "# then embed the clips using HeAR. Ignore clips with no detections\n",
        "LABELS_TO_EMBED = ['Cough', 'Snore', 'Breathe', 'Sneeze']\n",
        "# Assert that all labels to embed are actually present in the main label list\n",
        "assert all(label in LABEL_LIST for label in LABELS_TO_EMBED)\n",
        "\n",
        "# Clips of length CLIP_DURATION seconds are extracted from the audio file\n",
        "# using a sliding window. Adjecent clips are overlapped by CLIP_OVERLAP_PERCENT.\n",
        "CLIP_OVERLAP_PERCENT = 10\n",
        "\n",
        "# Labels must have score above this threshold to be considered a detection\n",
        "DETECTION_THRESHOLD = 0.9\n",
        "\n",
        "frame_length = int(CLIP_DURATION * SAMPLE_RATE)\n",
        "frame_step = int(frame_length * (1 - CLIP_OVERLAP_PERCENT / 100))\n",
        "hear_embeddings = {}\n",
        "for file_key, file_url in files_map.items():\n",
        "  hear_embeddings[file_key] = {}\n",
        "  print(f\"\\nLoading file: {file_key} from {file_url}\")\n",
        "  audio, sample_rate = librosa.load(file_key, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "  # Display full audio file (optional).\n",
        "  if SHOW_WAVEFORM:\n",
        "    plot_waveform(audio, sample_rate, title=file_key, color='blue')\n",
        "  if SHOW_SPECTROGRAM:\n",
        "    plot_spectrogram(\n",
        "      audio, sample_rate, title=file_key, n_fft=2*1024, hop_length=64, n_mels=256, cmap=COLORMAP)\n",
        "  if SHOW_PLAYER:\n",
        "    display(Audio(data=audio, rate=sample_rate))\n",
        "\n",
        "  # Segment an audio array into fixed length overlapping clips.\n",
        "  if len(audio) \u003c frame_length:\n",
        "    audio = np.pad(audio, (0, frame_length - len(audio)), mode='constant')\n",
        "  audio_clip_batch = tf.signal.frame(audio, frame_length, frame_step )\n",
        "  print(f\"Number of audio clips in batch: {len(audio_clip_batch)}.\")\n",
        "\n",
        "  # Perform detector inference on the audio_clip_batch\n",
        "  # The model will generate the input feature, then infer the detection\n",
        "  print(f\"Running batched {EVENT_DETECTOR} model inference on audio clips.\")\n",
        "  detection_scores_batch = event_detector(audio_clip_batch)[\"scores\"].numpy()\n",
        "  print(\"Computed batch probability scores with shape:\",\n",
        "        f\"{detection_scores_batch.shape} from input audio clips batch with\",\n",
        "        f\"shape: {audio_clip_batch.shape}\"\n",
        "  )\n",
        "  hear_embeddings[file_key]['detections'] = detection_scores_batch\n",
        "\n",
        "  if SHOW_DETECTION_SCORES:\n",
        "    plot_detection_scores(detection_scores_batch, LABEL_LIST, title=f'{file_key}: {EVENT_DETECTOR}', cmap=COLORMAP)\n",
        "\n",
        "  # Filter clips for HeAR inference based on if in ANY detection scores for\n",
        "  # 'LABELS_TO_EMBED' are above the 'DETECTION_THRESHOLD'.\n",
        "  print(f\"Filtering clips based on detections for labels: {LABELS_TO_EMBED}\")\n",
        "  embed_hear_clips = []\n",
        "  for clip_i, scores in enumerate(detection_scores_batch):\n",
        "    for label_index, label in enumerate(LABEL_LIST):\n",
        "      if label in LABELS_TO_EMBED and scores[label_index] \u003e DETECTION_THRESHOLD:\n",
        "        embed_hear_clips.append(audio_clip_batch[clip_i])\n",
        "        break\n",
        "\n",
        "  # Perform HeAR batch inference to extract the associated clip embedding.\n",
        "  # Only run inference on 'embed_hear_clips' which have have a high detection\n",
        "  # score for one of the 'LABELS_TO_EMBED'.\n",
        "  if len(embed_hear_clips):\n",
        "    print(f\"Computing HeAR embedding for batch of\",\n",
        "          f\"{len(embed_hear_clips)} selected clips.\")\n",
        "    hear_embedding_batch = hear_infer(x=np.asarray(embed_hear_clips))[ 'output_0'].numpy()\n",
        "    print(f\"Embedding batch shape: {hear_embedding_batch.shape},\",\n",
        "          f\"data type: {hear_embedding_batch.dtype}\")\n",
        "  else:\n",
        "    hear_embedding_batch = np.array([])\n",
        "    print(f\"None of the {len(audio_clip_batch)} clips in {file_key} have\",\n",
        "          f\"detections above the threshold: {DETECTION_THRESHOLD} for the\",\n",
        "          f\"labels: {LABELS_TO_EMBED}\")\n",
        "  hear_embeddings[file_key]['embeddings'] = hear_embedding_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFLdq8WI3AJT"
      },
      "outputs": [],
      "source": [
        "# @title Use HeAR embeddings to find most similar file to query file\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Set up query file and make sure if the query file_key exists in the dictionary and has embeddings.\n",
        "query_file_key = 'Cough_1.ogg'\n",
        "assert query_file_key in hear_embeddings and len(hear_embeddings[query_file_key]['embeddings'])\n",
        "\n",
        "# Get the average embedding for the query file and compare similarity to the average embedding for the other files.\n",
        "query_embedding = np.mean(hear_embeddings[query_file_key]['embeddings'], axis=0)\n",
        "similarities = {}\n",
        "for file_key, model_outputs in hear_embeddings.items():\n",
        "  # Skip comparing file_key to itself or comparing to keys without HeAR embeddings.\n",
        "  if file_key == query_file_key or not len(model_outputs['embeddings']):\n",
        "    continue\n",
        "\n",
        "  # Compute cosine similarity between the query file and the current file.\n",
        "  current_embedding = np.mean(model_outputs['embeddings'], axis=0)\n",
        "  similarities[file_key] = 1 - distance.cosine(query_embedding, current_embedding)\n",
        "\n",
        "# Find the top N most similar entries\n",
        "N = 3\n",
        "top_N_similar = dict(sorted(similarities.items(), key=lambda item: item[1], reverse=True)[:N])\n",
        "print(f\"\\nTop {N} most similar entries to '{query_file_key}':\")\n",
        "for key, similarity in top_N_similar.items():\n",
        "    print(f\"  {key}: {similarity:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4LoPrUCs0F4"
      },
      "source": [
        "# Extract Batch Frontend Spectrogram Features\n",
        "\n",
        "We provide a standalone, frozen frontend spectrogram model for efficient generation of [PCEN Mel-Spectrogram](https://research.google/pubs/trainable-frontend-for-robust-and-far-field-keyword-spotting/) features, which are used by the event detectors. This model is comprised of non-trainable TensorFlow operations, prioritizing portability, scalability, and optimization at the cost of configurability.\n",
        "\n",
        "The frontend can also be used as a standalone feature extractor for generating large amounts of training data for finetuning or retraining additional models as demonstrated below.\n",
        "\n",
        "The input must be 2-seconds and corresponding output feature will have shape `(200, 48)`. Typically, input clips will be segmented with some amount of overlap to avoid distorting sounds near the boundry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLDc2OFAngcT"
      },
      "outputs": [],
      "source": [
        "# @title Plot Example Frontend Features\n",
        "example_file_key =  'Sneezing.ogg'\n",
        "audio, sample_rate = librosa.load(example_file_key, sr=SAMPLE_RATE, mono=True)\n",
        "print(f\"Loaded {example_file_key} with duration {len(audio)/sample_rate:0.2f}s\")\n",
        "\n",
        "# Extract example clips with 50% overlap, then generate frontend feature batch.\n",
        "frame_length = int(CLIP_DURATION * SAMPLE_RATE)\n",
        "audio_clip_batch = tf.signal.frame(audio, frame_length, frame_length // 2 )\n",
        "frontend_feature_batch = frontend_model(audio_clip_batch)\n",
        "print(f\"Generated input feature batch from {example_file_key} with shape: {frontend_feature_batch.shape}\")\n",
        "for clip_i, frontend_feature in enumerate(frontend_feature_batch):\n",
        "  plot_frontend_feature(frontend_feature.numpy(), title=f'Clip: {clip_i}', cmap=COLORMAP, figsize=(8, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9e_m3Wf0ClK"
      },
      "source": [
        "# Convert Event Detectors to TFLite\n",
        "\n",
        "The included event detectors are based on [MobileNet-V3](https://huggingface.co/docs/timm/en/models/mobilenet-v3) and are designed to be run on-device with low latency and power requirements while also allowing for finetuning and quick training.\n",
        "\n",
        "Converting the event detector models to [LiteRT](https://ai.google.dev/edge/litert) (formally TFLite) allows for more optimal performance on specific hardware such as mobile devices or compute constrained servers.\n",
        "\n",
        "In order to save compute in realtime or large scale applications, these hardware optimized event detectors can be used as a gate for HeAR, only embedding clips detected to contain a health sound of interest (as shown above).\n",
        "\n",
        "The code below demonstrates converting the loaded event detector to a TensorFlow LiteRT model, there are many more options available to quantize or furthur optimize the converted models, see the [documentation](https://ai.google.dev/edge/litert/models/convert_tf) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5UimMm15FMy"
      },
      "outputs": [],
      "source": [
        "# @title TFLite Conversion\n",
        "%%time\n",
        "\n",
        "def convert_to_tflite(\n",
        "    model: tf.keras.Model,\n",
        "    quantize: bool = False,\n",
        ") -\u003e bytes:\n",
        "  \"\"\"Converts a SavedModel model to a TensorFlow Lite (TFLite) model.\n",
        "\n",
        "  Args:\n",
        "    model: The model to convert.\n",
        "    quantize: If True, apply dynamic range quantization to optimize the model.\n",
        "\n",
        "  Returns:\n",
        "    The raw byte representation of the converted TFLite model.\n",
        "  \"\"\"\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "  converter.target_spec.supported_ops = [\n",
        "      tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "      tf.lite.OpsSet.SELECT_TF_OPS,  # needed for frontend ops\n",
        "  ]\n",
        "  # See documentation for quantization options beyond dyanmic range quantization.\n",
        "  # https://ai.google.dev/edge/litert/models/post_training_quantization\n",
        "  if quantize:\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  return converter.convert()\n",
        "\n",
        "# Convert event detector to TFLite and save result.\n",
        "event_detector_lite = convert_to_tflite(event_detector, quantize=False)\n",
        "tflite_output_path ='event_detector.tflite'\n",
        "with open(tflite_output_path, 'wb') as f:\n",
        "  f.write(event_detector_lite)\n",
        "print(f\"Saved TFLite model to: {tflite_output_path}\")\n",
        "\n",
        "# Initalize TFLite Model and print I/O specification.\n",
        "tflite_interp = tf.lite.Interpreter(model_content=event_detector_lite)\n",
        "print(f\"Input details:\\n {tflite_interp.get_input_details()[0]}\")\n",
        "print(f\"Output details: \\n {tflite_interp.get_output_details()[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTxQttKYNpa"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/hear/blob/master/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//experimental/health_foundation_models/colab:colab_deps",
        "kind": "private"
      },
      "name": "hear_event_detector_demo.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
