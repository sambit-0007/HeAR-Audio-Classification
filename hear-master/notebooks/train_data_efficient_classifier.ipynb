{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Classifying sounds with HeAR and Wiki Commons Cough Data\n",
        "\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://colab.research.google.com/github/google-health/hear/blob/master/notebooks/train_data_efficient_classifier.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"\u003e\u003cbr\u003e Run in Google Colab\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e  \n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/google-health/hear/blob/master/notebooks/train_data_efficient_classifier.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://huggingface.co/google/hear\"\u003e\n",
        "      \u003cimg alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"\u003e\u003cbr\u003e View on HuggingFace\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n",
        "\n",
        "\n",
        "This Colab notebook demonstrates how to use the HeAR (Health Acoustic Representations) model, directly from Hugging Face, to create and utilize embeddings from health-related audio. The notebook focuses on building a data-efficient cough classifier system using a small [Wikimedia Commons](https://commons.wikimedia.org/wiki/Commons:Welcome) dataset of relevant sounds.\n",
        "\n",
        "Embeddings are compact, numerical representations of audio data that capture important features, making them suitable for training machine learning models with limited data and computational resources. Learn more about embeddings and their benefits at [this page](https://developers.google.com/health-ai-developer-foundations/hear).\n",
        "\n",
        "#### Here's a breakdown of the notebook's steps:\n",
        "\n",
        "1.  **Model Loading:** The HeAR model is loaded from the Hugging Face Hub (requires authentication with your Hugging Face account).\n",
        "\n",
        "2.  **Dataset Creation:**\n",
        "    *   **Wikimedia Commons Audio:** A small set of audio files is downloaded from Wikimedia Commons. This dataset includes examples of coughing, as well as other sounds like sneezing, breathing, laughter, and door knocking. The files are all publicly available under various Creative Commons licenses (details are available on Wikimedia Commons).\n",
        "    *   **Microphone Recording:** The notebook provides functionality to record audio directly within Colab using your microphone. This allows you to add your own recordings to the dataset.\n",
        "\n",
        "3.  **Embedding Generation:**\n",
        "    *   **Preprocessing:** The downloaded and recorded audio files are loaded and processed using `librosa`. They are resampled to 16kHz (required by the HeAR model) and segmented into 2-second clips.\n",
        "    *   **Inference:** The preprocessed 2-second audio clips are fed to the HeAR model to generate embeddings. Each clip produces a 512-dimensional HeAR embedding vector.\n",
        "    *   **Visualization (Optional):** The notebook includes functions to display the audio waveform, Mel spectrogram, and an audio player for each file and its individual clips.\n",
        "\n",
        "4.  **Classifier Training:**\n",
        "    *   **Labeling:** A set of labels is manually created, associating each audio file with whether it contains a cough or not. For example, `Cough_1.ogg` is labeled as `True`, while `Laughter.ogg` is labeled as `False`.\n",
        "    *   **Model Selection:** Several scikit-learn classifiers are used and can easily be expanded, including:\n",
        "        *   Support Vector Machine (linear kernel)\n",
        "        *   Logistic Regression\n",
        "        *   Gradient Boosting\n",
        "        *   Random Forest\n",
        "        *   Multi-layer Perceptron (MLP)\n",
        "    *   **Training:** Each classifier is trained using the generated HeAR embeddings and the corresponding cough labels. This demonstrates the data efficiency of using embeddings â€“ these models train quickly with very little data.\n",
        "\n",
        "5.  **Cough Classification:**\n",
        "    *   **Test on New Example:** Test the classfier on held out cough or non-cough sound examples.\n",
        "    *   **Test on New Recording:** The microphone recording function is used again to capture a new audio clip (presumably of the user coughing or not coughing).\n",
        "    *   **Prediction:** The new clip is preprocessed, its embedding is generated using the HeAR model, and then each of the trained classifiers is used to predict whether the clip contains a cough.\n",
        "\n",
        "6.  **Embedding Visualization:**\n",
        "    *   **PCA Plot:** A plot visualizing the data points in a PCA space is presented to show how similar sounds are grouped together, as they have similar embeddings.\n",
        "    *   **Barcode Visualization:** The embeddings are visualized as \"barcodes\". Each embedding is displayed as a row in a heatmap, showing the magnitude of each dimension after subtracting the global mean. This provides a visual representation of the embedding's structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uZFXCSuqr1V"
      },
      "source": [
        "## Authenticate with HuggingFace, skip if you have a HF_TOKEN secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-5Tj0uqS3dI"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub.utils import HfFolder\n",
        "\n",
        "if HfFolder.get_token() is None:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40C0ubVPS3dI"
      },
      "source": [
        "## Setup HeAR Hugging Face Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqB4-dSUeQKe"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import from_pretrained_keras\n",
        "\n",
        "# Load the model directly from Hugging Face Hub\n",
        "loaded_model = from_pretrained_keras(\"google/hear\")\n",
        "# Inference function for embedding generation\n",
        "infer = loaded_model.signatures[\"serving_default\"]\n",
        "\n",
        "# HeAR Parameters\n",
        "SAMPLE_RATE = 16000  # Samples per second (Hz)\n",
        "CLIP_DURATION = 2    # Duration of the audio clip in seconds\n",
        "CLIP_LENGTH = SAMPLE_RATE * CLIP_DURATION  # Total number of samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1KzmL_kqbRz"
      },
      "outputs": [],
      "source": [
        "# @title Test Model Inference on Random Input\n",
        "%%time\n",
        "import numpy as np\n",
        "\n",
        "# Generate Random Input Audio\n",
        "NUM_EXAMPLES = 4  # number of random audio examples to generate\n",
        "print(f\"Generating {NUM_EXAMPLES} {CLIP_DURATION}s raw audio examples.\")\n",
        "raw_audio = np.random.normal(size=(NUM_EXAMPLES, CLIP_LENGTH))\n",
        "print(f\"Raw audio shape: {raw_audio.shape}, data type: {raw_audio.dtype}\\n\")\n",
        "\n",
        "# Perform Inference Extract and Process the Embedding\n",
        "print(f'Running HeAR model to produce {NUM_EXAMPLES} embeddings.')\n",
        "output_dict = infer(x=raw_audio)\n",
        "embedding = output_dict['output_0'].numpy()  # directly unpack as a NumPy array\n",
        "print(f\"Embedding shape: {embedding.shape}, data type: {embedding.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO-Z5BOtj3D1"
      },
      "source": [
        "## Download and Record Audio Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZnJwzzj3D1"
      },
      "source": [
        " Wiki Commons\n",
        "https://commons.wikimedia.org/wiki/Category:Coughing_audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ55XsJsj3D2"
      },
      "outputs": [],
      "source": [
        "# @title Download Public Domain Cough Examples to Notebook\n",
        "import os\n",
        "import subprocess\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# More examples: https://commons.wikimedia.org/wiki/Category:Coughing_audio\n",
        "wiki_cough_file_urls = [\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/c/cc/Man_coughing.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/6/6a/Cough_1.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/d/d9/Cough_2.ogg', # hold out for test\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/b/be/Woman_coughing_three_times.wav',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/d/d0/Sneezing.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/b/bc/Windy_breath.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/e/ef/Laughter_and_clearing_voice.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/c/c6/Laughter.ogg',\n",
        "  'https://upload.wikimedia.org/wikipedia/commons/1/1c/Knocking_on_wood_or_door.ogg',\n",
        "]\n",
        "\n",
        "# Download the files.\n",
        "files_map = {}  # file name to file path map\n",
        "file_embeddings = {} # embedding cache\n",
        "for url in wiki_cough_file_urls:\n",
        "    filename = os.path.basename(urlparse(url).path)\n",
        "    print(f'Downloading {filename}...')\n",
        "    res = subprocess.run(['wget', '-nv', '-O', filename, url], capture_output=True, text=True)\n",
        "    if res.returncode != 0:\n",
        "        print(f\"  Download failed. Return code: {res.returncode}\\nError: {res.stderr}\")\n",
        "    files_map[filename] = url\n",
        "print(f'\\nLocal Files:\\n{os.listdir():}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "28haI1t7Iq4X"
      },
      "outputs": [],
      "source": [
        "# @title Microphone Helpers\n",
        "from io import BytesIO\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript\n",
        "\n",
        "RECORD_JAVASCRIPT = \"\"\"\n",
        "const sleep  = time =\u003e new Promise(resolve =\u003e setTimeout(resolve, time))\n",
        "const b2text = blob =\u003e new Promise(resolve =\u003e {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e =\u003e resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time =\u003e new Promise(async resolve =\u003e {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e =\u003e chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=\u003e{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "def record_microphone_and_save(duration_seconds=2, filename=\"output_audio\", extension='.webm'):\n",
        "  output_filename = filename + extension\n",
        "  print(f\"\\nRecording for {duration_seconds} seconds...\")\n",
        "  display(Javascript(RECORD_JAVASCRIPT))\n",
        "  base64_audio = output.eval_js('record(%d)' % (duration_seconds * 1000))\n",
        "  print(\"Done Recording!\")\n",
        "  audio_bytes = b64decode(base64_audio.split(',')[1])\n",
        "\n",
        "  # Save the audio to a file\n",
        "  with open(output_filename, 'wb') as file:\n",
        "      file.write(audio_bytes)\n",
        "  print(f\"Audio saved as {output_filename}\")\n",
        "  return output_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtMcXr3o6gxN"
      },
      "outputs": [],
      "source": [
        "# @title Record your own file\n",
        "\n",
        "recording_name = \"my_recording\" # will overwrite existing\n",
        "recording_file = record_microphone_and_save(duration_seconds=CLIP_DURATION, filename=recording_name)\n",
        "files_map[recording_file] = recording_name # add to file map from above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz5Z4BOlGVHm"
      },
      "source": [
        "## Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "33G4zKJHjGGc"
      },
      "outputs": [],
      "source": [
        "# @title Plot Helpers\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "\n",
        "# Suppress the specific warning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"soundfile\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"librosa\")\n",
        "\n",
        "\n",
        "def plot_waveform(sound, sr, title, figsize=(12, 4), color='blue', alpha=0.7):\n",
        "  \"\"\"Plots the waveform of the audio using librosa.display.\"\"\"\n",
        "  plt.figure(figsize=figsize)\n",
        "  librosa.display.waveshow(sound, sr=sr, color=color, alpha=alpha)\n",
        "  plt.title(f\"{title}\\nshape={sound.shape}, sr={sr}, dtype={sound.dtype}\")\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Amplitude\")\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_spectrogram(sound, sr, title, figsize=(12, 4), n_fft=2048, hop_length=256, n_mels=128, cmap='nipy_spectral'):\n",
        "  \"\"\"Plots the Mel spectrogram of the audio using librosa.\"\"\"\n",
        "  plt.figure(figsize=figsize)\n",
        "  mel_spectrogram = librosa.feature.melspectrogram(y=sound, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "  log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "  librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap=cmap)\n",
        "  plt.title(f\"{title} - Mel Spectrogram\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYbz904QXio2"
      },
      "outputs": [],
      "source": [
        "# @title Load Audio and Generate HeAR Embeddings\n",
        "%%time\n",
        "\n",
        "# Audio display options\n",
        "SHOW_WAVEFORM = False\n",
        "SHOW_SPECTROGRAM = True\n",
        "SHOW_PLAYER = True\n",
        "SHOW_CLIPS = False\n",
        "\n",
        "# Clips of length CLIP_DURATION seconds are extracted from the audio file\n",
        "# using a sliding window. Adjecent clips are overlapped by CLIP_OVERLAP_PERCENT.\n",
        "CLIP_OVERLAP_PERCENT = 10\n",
        "\n",
        "# When True, if a clip extracted from the file is quieter than\n",
        "# the SILENCE_RMS_THRESHOLD_DB it is not sent to the HeAR model.\n",
        "CLIP_IGNORE_SILENT_CLIPS = True\n",
        "# Maximum average amplitude of a frame to be considered silence.\n",
        "SILENCE_RMS_THRESHOLD_DB = -50\n",
        "\n",
        "\n",
        "for file_key, file_url in files_map.items():\n",
        "  # Load the audio file into numpy array with specified sample rate and 1 channel (mono).\n",
        "  print(f\"\\nLoading file: {file_key} from {file_url}\")\n",
        "  audio, sample_rate = librosa.load(file_key, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "  # Display audio file (optional)\n",
        "  if SHOW_WAVEFORM:\n",
        "    plot_waveform(audio, sample_rate, title=file_key, color='blue')\n",
        "  if SHOW_SPECTROGRAM:\n",
        "    plot_spectrogram(audio, sample_rate, file_key,  n_fft=2*1024, hop_length=64, n_mels=256, cmap='Blues')\n",
        "  if SHOW_PLAYER:\n",
        "    display(Audio(data=audio, rate=sample_rate))\n",
        "\n",
        "  # This code segments an audio array into overlapping clips.\n",
        "  # It calculates the number of clips, iterates through them,\n",
        "  # and handles potential padding with zeros for the last clip if needed.\n",
        "  clip_batch = []\n",
        "  overlap_samples = int(CLIP_LENGTH * (CLIP_OVERLAP_PERCENT / 100))\n",
        "  step_size = CLIP_LENGTH - overlap_samples\n",
        "  num_clips = max(1, (len(audio) - overlap_samples) // step_size)\n",
        "  print(f\" Segmenting into {num_clips} {CLIP_DURATION}s clips\")\n",
        "  for i in range(num_clips):\n",
        "    start_sample = i * step_size\n",
        "    end_sample = start_sample + CLIP_LENGTH\n",
        "    clip = audio[start_sample:end_sample]\n",
        "    # Pad clip with zeros if less than the required CLIP_LENGTH.\n",
        "    if end_sample \u003e len(audio):\n",
        "        print(\"  Last clip: Padding with zeros.\")\n",
        "        clip = np.pad(clip, (0, CLIP_LENGTH - len(clip)), 'constant')\n",
        "    # Average Loudness of the clip(in dB)\n",
        "    rms_loudness =  round(20 * np.log10(np.sqrt(np.mean(clip**2))))\n",
        "\n",
        "    # Display clip info (optional)\n",
        "    clip_str = f\"Clip {i+1} from {file_key} [loudness: {rms_loudness} dB]\"\n",
        "    print(f\"  {clip_str}\")\n",
        "    if SHOW_CLIPS:\n",
        "      if SHOW_WAVEFORM:\n",
        "        plot_waveform(clip, sample_rate, title=clip_str, figsize=(8, 3), color=cm.rainbow(i /num_clips))\n",
        "      if SHOW_PLAYER:\n",
        "        display(Audio(data=clip, rate=sample_rate))\n",
        "\n",
        "    # Skip if clip is too quiet\n",
        "    if CLIP_IGNORE_SILENT_CLIPS and rms_loudness \u003c SILENCE_RMS_THRESHOLD_DB:\n",
        "      print(f\"  Clip {i+1} Skip...too quiet [loudness: {rms_loudness} dB]\")\n",
        "      continue\n",
        "\n",
        "    # Add clip to batch\n",
        "    clip_batch.append(clip)\n",
        "\n",
        "\n",
        "  # Perform HeAR Batch inference to extract the associated clip embedding.\n",
        "  # Only run inference if embedding not already in file_embedding cache.\n",
        "  clip_batch = np.asarray(clip_batch)\n",
        "  if file_key not in file_embeddings:\n",
        "    print(\"  Clip not in cache, performing inference...\")\n",
        "    embedding_batch = infer(x=clip_batch)['output_0'].numpy()\n",
        "    file_embeddings[file_key] = embedding_batch\n",
        "  else:\n",
        "    embedding_batch = file_embeddings[file_key]\n",
        "  print(f\"  Embedding batch shape: {embedding_batch.shape}, data type: {embedding_batch.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrIP1KKIymtu"
      },
      "outputs": [],
      "source": [
        "# @title Collect Embeddings, Create Training Set\n",
        "\n",
        "# Hold out example for testing with cough classifier later.\n",
        "# Only using one test file for now, can also record your own.\n",
        "# Note: `Cough_2.ogg` is likely the same person as `Cough_1.ogg`,\n",
        "#  so we expect it to produce a similar embedding\n",
        "test_file = 'Cough_2.ogg'\n",
        "assert test_file in files_map, f\"Test file '{test_file}' not found in files_map.\"\n",
        "\n",
        "\n",
        "# Combine train embeddings and hold out test embeddings.\n",
        "test_embeddings, test_file_names = [], [] # held out\n",
        "train_embeddings, train_file_names = [], []\n",
        "for file_key, embedding_batch in file_embeddings.items():\n",
        "  for embedding in embedding_batch:\n",
        "    if file_key == test_file:\n",
        "      test_embeddings.append(embedding)\n",
        "      test_file_names.append(file_key)\n",
        "    else:\n",
        "      train_embeddings.append(embedding)\n",
        "      train_file_names.append(file_key)\n",
        "train_embeddings = np.array(train_embeddings)\n",
        "train_file_set = set(train_file_names)\n",
        "test_file_set = set(test_file_names)\n",
        "\n",
        "print(f\"Train embeddings have shape: {train_embeddings.shape}, data type: {train_embeddings.dtype}\")\n",
        "print(f\"Train Embeddings are from {len(train_file_set)} unique files:{train_file_set}\")\n",
        "print(f\"Test Embeddings are from {len(test_file_set)} unique files:{test_file_set}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkr8EXfOGgHm"
      },
      "source": [
        "## Use Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVo9WYSYjZ6H"
      },
      "outputs": [],
      "source": [
        "# @title Plot Train Embeddings, show average Embedding per file\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_embeddings = pca.fit_transform(train_embeddings)\n",
        "\n",
        "# Calculate average embedding per file after PCA, mark as star\n",
        "avg_embeddings_per_file_pca = {}\n",
        "for file_key in train_file_set:\n",
        "    file_indices = [i for i, key in enumerate(train_file_names) if key == file_key]\n",
        "    avg_embeddings_per_file_pca[file_key] = np.mean(pca_embeddings[file_indices], axis=0)\n",
        "\n",
        "# Plot with coloring and average embedding\n",
        "plt.figure(figsize=(10, 10))\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(train_file_set)))\n",
        "color_map = {key: colors[i] for i, key in enumerate(train_file_set)}\n",
        "for i, embedding in enumerate(pca_embeddings):\n",
        "    file_key = train_file_names[i]\n",
        "    plt.scatter(embedding[0], embedding[1], color=color_map[file_key], alpha=0.5)  # No label for each dot\n",
        "\n",
        "# Add average embeddings as star markers (using PCA averages)\n",
        "for file_key, avg_embedding in avg_embeddings_per_file_pca.items():\n",
        "    plt.scatter(avg_embedding[0], avg_embedding[1], marker='*', color=color_map[file_key], label=file_key, s=400)  # Label for average\n",
        "\n",
        "plt.xlabel(\"PCA Dimension 1\")\n",
        "plt.ylabel(\"PCA Dimension 2\")\n",
        "plt.title(\"Embeddings with PCA, File Coloring, and PCA Average Embedding Markers\")\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEuQRsS3D52y"
      },
      "outputs": [],
      "source": [
        "# @title Train a few-shot cough classifier with HeAR embeddings\n",
        "%%time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "# True if file has coughing (not perfect as some parts of file may not have coughing)\n",
        "file_cough_labels = {\n",
        "    'Laughter.ogg': False,\n",
        "    'Cough_1.ogg': True,\n",
        "    'Windy_breath.ogg': False,\n",
        "    'Man_coughing.ogg': True,\n",
        "    'Cough_2.ogg': True,\n",
        "    'Woman_coughing_three_times.wav': True,\n",
        "    'Short_coughs.ogg': True,\n",
        "    'Laughter_and_clearing_voice.ogg': False,\n",
        "    'Sneezing.ogg': False,\n",
        "    'Knocking_on_wood_or_door.ogg': False,\n",
        "    'recording.webm': True,\n",
        "    'my_recording.webm': True,\n",
        "\n",
        "}\n",
        "cough_labels = []\n",
        "for file_name in train_file_names:\n",
        "  if file_name in file_cough_labels:\n",
        "    cough_labels.append(1 if file_cough_labels[file_name] else 0)\n",
        "  elif \"cough\" in file_name.lower():\n",
        "    cough_labels.append(1)\n",
        "  else:\n",
        "    cough_labels.append(0)\n",
        "    print(f\"Warning: No label found for '{file_name}'. Defaulting to False.\")\n",
        "\n",
        "# Train more powerful classifier models\n",
        "models = {\n",
        "    \"Support Vector Machine (linear)\": SVC(kernel='linear'),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=128),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=128),\n",
        "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(128, 64)),\n",
        "}\n",
        "\n",
        "cough_models = {}\n",
        "for name, model in models.items():\n",
        "  model.fit(train_embeddings, cough_labels)\n",
        "  cough_models[name] = model\n",
        "  print(f\"Finished training: {name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJuJFxrwHXL3"
      },
      "outputs": [],
      "source": [
        "# @title Classify Held Out Test Example\n",
        "\n",
        "print(f\"Classifying {len(test_embeddings)} embeddings from {test_file} with the {len(cough_models)} models...\")\n",
        "for model_name, cough_model in cough_models.items():\n",
        "  # Note: Since the clip is divided into CLIP_DURATION length subclips, some\n",
        "  # clips will contain the cough while others wont. Since we want to know if\n",
        "  # ANY clip from this test file contains a cough, can check max(predcitions).\n",
        "  # If we want to know where in the clip the coughs occur we can look at\n",
        "  # which clip indices classified 1 (cough)\n",
        "  prediction = cough_model.predict(test_embeddings).max()\n",
        "  print(f\" {model_name} Classification: {'Cough' if prediction == 1 else 'No Cough'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_sPZY4I_gbV"
      },
      "outputs": [],
      "source": [
        "# @title Record and Classify Cough\n",
        "recording_filename = \"my_test_cough\" # will overwrite existing\n",
        "recording_file = record_microphone_and_save(duration_seconds=CLIP_DURATION, filename=recording_filename)\n",
        "recording_clip = librosa.load(recording_file, sr=SAMPLE_RATE)[0]\n",
        "print(f\"Loaded Test file {recording_filename}, audio has shape: {recording_clip.shape}\")\n",
        "\n",
        "print(f\"Generate HeAR embedding for {recording_filename}\")\n",
        "# Note: Since the recording is exactly CLIP_DURATION seconds, we will have a\n",
        "# single clip and produce a single embedding.\n",
        "recording_batch = np.expand_dims(np.pad(recording_clip, (0, CLIP_LENGTH - len(recording_clip)), 'constant'), axis=0)\n",
        "recording_embedding = infer(x=recording_batch)['output_0'].numpy()\n",
        "print(f\"Embedding has shape: {recording_embedding.shape}\")\n",
        "\n",
        "# Classify recorded file with each classifier.\n",
        "print(f\"\\nClassifying test file: {recording_filename} using {len(cough_models)} models...\")\n",
        "for model_name, cough_model in cough_models.items():\n",
        "  # Note: Similar to the above held out example, we will have a prediction for\n",
        "  # each clip within the file, in this case we have one clip from the recording.\n",
        "  prediction = cough_model.predict(recording_embedding).max() # or [0]\n",
        "  print(f\" {model_name} Classification: {'Cough' if prediction == 1 else 'No Cough'}\")\n",
        "\n",
        "# Player for recorded clip\n",
        "Audio(data=recording_clip, rate=SAMPLE_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zr2A1cZY0Hm"
      },
      "outputs": [],
      "source": [
        "# @title Plot Embeddings as Barcode Figures\n",
        "\n",
        "# Note: We subtract the mean embedding so plots highlight the differences.\n",
        "embedding_mean = np.mean(train_embeddings, axis=0)\n",
        "for file_key, embedding_batch in file_embeddings.items():\n",
        "  batch_size = embedding_batch.shape[0]\n",
        "  embedding_batch_norm = embedding_batch - embedding_mean\n",
        "  print(f\"{file_key} has {batch_size} embeddings...\")\n",
        "\n",
        "  plt.figure(figsize=(18, 1 * embedding_batch.shape[0]))\n",
        "  for i in range(batch_size):\n",
        "    embedding_magnitude = embedding_batch_norm[i, :] ** 2\n",
        "    plt.subplot(batch_size, 1, i + 1)\n",
        "    plt.imshow(embedding_magnitude.reshape(1, -1), cmap='binary',  interpolation=None, aspect='auto')\n",
        "    plt.title(f\"Embedding {i+1}, File: {file_key}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTxQttKYNpa"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/hear/blob/master/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "train_data_efficient_classifier.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
