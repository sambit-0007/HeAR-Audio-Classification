# Health Acoustic Representations (HeAR)

Health Acoustic Representations (HeAR) is a machine learning (ML) model that
produces embeddings based on health acoustics. The embeddings can be used to
efficiently build AI models for health accoustic related tasks, requiring less
data and less compute than having to fully train a model without the embeddings.

As a Health AI Developer Foundations model trained on 300+ million two-second
long audio clips, HeAR accelerates their ability to build AI models for health
acoustic analysis with less data and compute.

## Get started

*   Read our
    [developer documentation](https://developers.google.com/health-ai-developer-foundations/hear/get-started)
    to see the full range of next steps available, including learning more about
    the model through its
    [model card](https://developers.google.com/health-ai-developer-foundations/hear/model-card)
    or
    [serving API](https://developers.google.com/health-ai-developer-foundations/hear/serving-api).

*   Explore this repository, which contains [notebooks](./notebooks) for using
    the model from Hugging Face and Vertex AI as well as the
    [implementation](./python/serving) of the container that you can deploy to
    Vertex AI.

*   Visit the model on
    [Hugging Face](https://huggingface.co/google/hear) or
    [Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/hear).

## Contributing

We are open to bug reports, pull requests (PR), and other contributions. See
[CONTRIBUTING](CONTRIBUTING.md) and
[community guidelines](https://developers.google.com/health-ai-developer-foundations/community-guidelines)
for details.

## License

While the model is licensed under the
[Health AI Developer Foundations License](https://developers.google.com/health-ai-developer-foundations/terms),
everything in this repository is licensed under the Apache 2.0 license, see
[LICENSE](LICENSE).
